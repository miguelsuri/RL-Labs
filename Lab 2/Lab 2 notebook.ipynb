{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "button": false,
        "id": "20IyxDzgp3tU",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt # Graphical library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyuTicLD6F2z"
      },
      "source": [
        "# Lab Assignment 2 :  \n",
        "See pdf for instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIYexixg588i"
      },
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZWnMW3GNpjd7"
      },
      "outputs": [],
      "source": [
        "# This class is used ONLY for graphics\n",
        "# YOU DO NOT NEED to understand it to work on this lab assignment\n",
        "\n",
        "class GraphicsGridWorld(object):\n",
        "\n",
        "  def __init__(self, shape, locations, obstacle_locs, absorbing_locs, absorbing_rewards, absorbing):\n",
        "\n",
        "    # Ensure all inputs are valids using the is_valid_grid function\n",
        "    is_valid_grid(shape, 0, obstacle_locs, absorbing_locs, absorbing_rewards)\n",
        "\n",
        "    self.shape = shape\n",
        "    self.locations = locations\n",
        "    self.absorbing = absorbing\n",
        "\n",
        "    # Walls\n",
        "    self.walls = np.zeros(self.shape)\n",
        "    for ob in obstacle_locs:\n",
        "      self.walls[ob] = 1\n",
        "\n",
        "    # Absorbing states\n",
        "    self.absorbers = np.zeros(self.shape)\n",
        "    for ab in absorbing_locs:\n",
        "      self.absorbers[ab] = -1\n",
        "\n",
        "    # Rewards\n",
        "    self.rewarders = np.zeros(self.shape)\n",
        "    for i, rew in enumerate(absorbing_locs):\n",
        "      self.rewarders[rew] = absorbing_rewards[i]\n",
        "\n",
        "    # Print the map to show it\n",
        "    self.paint_maps()\n",
        "\n",
        "  def paint_maps(self):\n",
        "    \"\"\"\n",
        "    Print the Grid topology (obstacles, absorbing states and rewards)\n",
        "    input: /\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(self.walls)\n",
        "    plt.title('Obstacles')\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(self.absorbers)\n",
        "    plt.title('Absorbing states')\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(self.rewarders)\n",
        "    plt.title('Reward states')\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a deterministic policy\n",
        "    input: Policy {np.array} -- policy to draw (should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(self.walls + self.rewarders + self.absorbers) # Create the graph of the grid\n",
        "    for state, action in enumerate(Policy):\n",
        "      if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "        continue\n",
        "      arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "      action_arrow = arrows[action] # Take the corresponding action\n",
        "      location = self.locations[state] # Compute its location on graph\n",
        "      plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy(self, Policy):\n",
        "    \"\"\"\n",
        "    Draw a policy (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- policy to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policy = np.array([np.argmax(Policy[row,:]) for row in range(Policy.shape[0])])\n",
        "    self.draw_deterministic_policy(deterministic_policy)\n",
        "\n",
        "  def draw_value(self, Value):\n",
        "    \"\"\"\n",
        "    Draw a policy value\n",
        "    input: Value {np.array} -- policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(self.walls + self.rewarders + self.absorbers) # Create the graph of the grid\n",
        "    for state, value in enumerate(Value):\n",
        "      if(self.absorbing[0, state]): # If it is an absorbing state, don't plot any value\n",
        "        continue\n",
        "      location = self.locations[state] # Compute the value location on graph\n",
        "      plt.text(location[1], location[0], round(value,2), ha='center', va='center') # Place it on graph\n",
        "    plt.show()\n",
        "\n",
        "  def draw_deterministic_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple deterministic policies\n",
        "    input: Policies {np.array of np.array} -- array of policies to draw (each should be an array of values between 0 and 3 (actions))\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Policies)): # Go through all policies\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each policy\n",
        "      ax.imshow(self.walls+self.rewarders +self.absorbers) # Create the graph of the grid\n",
        "      for state, action in enumerate(Policies[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any action\n",
        "          continue\n",
        "        arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"] # List of arrows corresponding to each possible action\n",
        "        action_arrow = arrows[action] # Take the corresponding action\n",
        "        location = self.locations[state] # Compute its location on graph\n",
        "        plt.text(location[1], location[0], action_arrow, ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graph given as argument\n",
        "    plt.show()\n",
        "\n",
        "  def draw_policy_grid(self, Policies, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policies (draw an arrow in the most probable direction)\n",
        "    input: Policy {np.array} -- array of policies to draw as probability\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    deterministic_policies = np.array([[np.argmax(Policy[row,:]) for row in range(Policy.shape[0])] for Policy in Policies])\n",
        "    self.draw_deterministic_policy_grid(deterministic_policies, title, n_columns, n_lines)\n",
        "\n",
        "  def draw_value_grid(self, Values, title, n_columns, n_lines):\n",
        "    \"\"\"\n",
        "    Draw a grid representing multiple policy values\n",
        "    input: Values {np.array of np.array} -- array of policy values to draw\n",
        "    output: /\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for subplot in range (len(Values)): # Go through all values\n",
        "      ax = plt.subplot(n_columns, n_lines, subplot+1) # Create a subplot for each value\n",
        "      ax.imshow(self.walls+self.rewarders +self.absorbers) # Create the graph of the grid\n",
        "      for state, value in enumerate(Values[subplot]):\n",
        "        if(self.absorbing[0,state]): # If it is an absorbing state, don't plot any value\n",
        "          continue\n",
        "        location = self.locations[state] # Compute the value location on graph\n",
        "        plt.text(location[1], location[0], round(value,1), ha='center', va='center') # Place it on graph\n",
        "      ax.title.set_text(title[subplot]) # Set the title for the graoh given as argument\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pMKhnaBrstG-"
      },
      "outputs": [],
      "source": [
        "# Here, we define some functions used across the code\n",
        "# YOU DO NOT NEED to understand it to work on this lab assignment\n",
        "\n",
        "def is_valid_grid(shape, prob_success, obstacle_locs, absorbing_locs, absorbing_rewards):\n",
        "  \"\"\"\n",
        "  Ensure the property defining the grid are all valid\n",
        "  inputs: all properties to initialise a GridWorld (see GridWorld class)\n",
        "  outputs: /\n",
        "  \"\"\"\n",
        "  assert len(shape) == 2, \"The grid should be two dimensions.\"\n",
        "  assert (prob_success <= 1) and (prob_success >= 0), \"Probability of action success should be in [0, 1].\"\n",
        "  for obstacle_loc in obstacle_locs:\n",
        "    assert len(obstacle_loc) == 2, \"The obstacle locations should have two coordinates.\"\n",
        "    assert (obstacle_loc[0] < shape[0]) and (obstacle_loc[1] < shape[1]), \"The obstacle locations should be inside the grid.\"\n",
        "  assert len(absorbing_locs) == len(absorbing_rewards), \"The absorbing_locs and absorbing_rewards should have the same length.\"\n",
        "  for absorbing_loc in absorbing_locs:\n",
        "    assert len(absorbing_loc) == 2, \"The absorbing locations should have two coordinates.\"\n",
        "    assert (absorbing_loc[0] < shape[0]) and (absorbing_loc[1] < shape[1]), \"The absorbing locations should be inside the grid.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOqA-RtjUn-_"
      },
      "source": [
        "## GridWorld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MXc1OFvZqJfZ"
      },
      "outputs": [],
      "source": [
        "# This class define the Grid world\n",
        "\n",
        "class GridWorld(object):\n",
        "\n",
        "  def __init__(self,\n",
        "               shape = (5,5), \n",
        "               prob_success = 0.7,\n",
        "               obstacle_locs = [(1,1),(2,1),(2,3)], \n",
        "               absorbing_locs = [(4,0),(4,1),(4,2),(4,3),(4,4)], \n",
        "               absorbing_rewards = [-10, -10, -10, -10, 10]  \n",
        "              ):\n",
        "    \"\"\"\n",
        "    GridWorld initialisation\n",
        "    input: \n",
        "      - shape {tuple} -- GridWorld shape (height, width)\n",
        "      - prob_success {float} -- probability of success when taking an action, used to fill the transition matrix\n",
        "      - obstacle_locs {list of tuples} -- location of all obstacles of the grid: [(obstacle 1), (obstacle 2), ...] \n",
        "      - absorbing_locs {list of tuples} -- location of all absorbing states of the grid: [(state 1), (state 2), ...]\n",
        "      - absorbing_rewards {list of float} -- reward corresponding to eacg absorbing state of the grid: [reward 1, reward 2, ...]\n",
        "    output: /\n",
        "    \"\"\"\n",
        "      \n",
        "    # Ensure all inputs are valids using the is_valid_grid function\n",
        "    is_valid_grid(shape, prob_success, obstacle_locs, absorbing_locs, absorbing_rewards)\n",
        "\n",
        "    # Setting inputs as attributes\n",
        "    self.shape = shape\n",
        "    self.prob_success = prob_success\n",
        "    self.obstacle_locs = obstacle_locs\n",
        "    self.absorbing_locs = absorbing_locs\n",
        "    self.absorbing_rewards = absorbing_rewards\n",
        "\n",
        "    # Actions\n",
        "    self.action_size = 4\n",
        "    self.direction_names = ['N','E','S','W'] # Direction 0 is 'N', 1 is 'E' and so on\n",
        "        \n",
        "    # States\n",
        "    self.locations = self.fill_in_states()\n",
        "    self.state_size = len(self.locations)\n",
        "    self.neighbours = self.fill_in_neighbours()\n",
        "    self.absorbing = self.fill_in_absorbing()\n",
        "\n",
        "    # Transition and reward matrices\n",
        "    self.T = self.fill_in_transition()\n",
        "    self.R = self.fill_in_reward()\n",
        "\n",
        "    # Creating the graphical grid world\n",
        "    self.graphics = GraphicsGridWorld(self.shape, self.locations, self.obstacle_locs, self.absorbing_locs, self.absorbing_rewards, self.absorbing)\n",
        "\n",
        "\n",
        "  def is_location(self, loc):\n",
        "    \"\"\"\n",
        "    Is the location a valid state (not out of grid and not an obstacle)\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: _ {bool} -- is the location a valid state\n",
        "    \"\"\"\n",
        "    if (loc[0] < 0 or loc[1] < 0 or loc[0] > self.shape[0]-1 or loc[1] > self.shape[1]-1):\n",
        "      return False\n",
        "    elif (loc in self.obstacle_locs):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "\n",
        "  def get_state_from_loc(self, loc):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: loc {tuple} -- location of the state\n",
        "    output: index {int} -- corresponding state number\n",
        "    \"\"\"\n",
        "    return self.locations.index(tuple(loc))\n",
        "\n",
        "\n",
        "  def get_loc_from_state(self, state):\n",
        "    \"\"\"\n",
        "    Get the state number corresponding to a given location\n",
        "    input: index {int} -- state number\n",
        "    output: loc {tuple} -- corresponding location\n",
        "    \"\"\"\n",
        "    return self.locations[state]\n",
        "\n",
        "\n",
        "  def fill_in_states(self):\n",
        "    \"\"\"\n",
        "    Build the states numbers\n",
        "    input: /\n",
        "    output: locations {list of tuples} -- mapping from location to state number\n",
        "    \"\"\"\n",
        "    \n",
        "    locations = []\n",
        "    for i in range (self.shape[0]):\n",
        "      for j in range (self.shape[1]):\n",
        "        loc = (i,j) \n",
        "        # Adding the state to locations if it is no obstacle\n",
        "        if self.is_location(loc):\n",
        "          locations.append(loc)\n",
        "    return locations\n",
        "\n",
        "\n",
        "  def fill_in_neighbours(self):\n",
        "    \"\"\"\n",
        "    Build the neighbouring states in the grid\n",
        "    input: /\n",
        "    output: neighbours {np.array} -- matrix containing the state number of the neighbours in each direction of all states\n",
        "    \"\"\"\n",
        "   \n",
        "    # Each line is a state, ranked by state-number, each column is a direction (N, E, S, W)\n",
        "    neighbours = np.zeros((self.state_size, 4)) \n",
        "    \n",
        "    for state in range(self.state_size):\n",
        "      loc = self.get_loc_from_state(state)\n",
        "\n",
        "      # North\n",
        "      neighbour = (loc[0]-1, loc[1]) # North neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('N')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('N')] = state\n",
        "\n",
        "      # East\n",
        "      neighbour = (loc[0], loc[1]+1) # East neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('E')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('E')] = state\n",
        "\n",
        "      # South\n",
        "      neighbour = (loc[0]+1, loc[1]) # South neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('S')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('S')] = state\n",
        "\n",
        "      # West\n",
        "      neighbour = (loc[0], loc[1]-1) # West neighbours location\n",
        "      if self.is_location(neighbour):\n",
        "        neighbours[state][self.direction_names.index('W')] = self.get_state_from_loc(neighbour)\n",
        "      else: # If there is no neighbour in this direction, coming back to current state\n",
        "        neighbours[state][self.direction_names.index('W')] = state\n",
        "\n",
        "    return neighbours\n",
        "\n",
        "\n",
        "  def fill_in_absorbing(self):\n",
        "    \"\"\"\n",
        "    Translate absorbing locations into absorbing state indices\n",
        "    input: /\n",
        "    output: absorbing {np.array} -- array with value 1 when the state is absorbing the state number of the neighbours in each direction of all states\n",
        "    \"\"\"\n",
        "    absorbing = np.zeros((1, self.state_size))\n",
        "    for a in self.absorbing_locs:\n",
        "      absorbing_state = self.get_state_from_loc(a)\n",
        "      absorbing[0, absorbing_state] = 1\n",
        "    return absorbing\n",
        "\n",
        "\n",
        "  # [Action required]\n",
        "  def fill_in_transition(self):\n",
        "    \"\"\"\n",
        "    Compute the transition matrix of the grid\n",
        "    input: /\n",
        "    output: T {np.array} -- the transition matrix of the grid\n",
        "    \"\"\"\n",
        "    # Empty matrix of dimension S*S*A\n",
        "    T = np.zeros((self.state_size, self.state_size, self.action_size)) \n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.action_size, self.state_size, self.neighbours, self.prob_success\n",
        "    ####\n",
        "\n",
        "    for state in range(self.state_size):\n",
        "      for neigh in self.neighbours[state]:\n",
        "        for act in range(self.action_size):\n",
        "          if neigh == act:\n",
        "            T[state][int(neigh)][act] = self.prob_success\n",
        "          else:\n",
        "            T[state][int(neigh)][act] = (1 - self.prob_success) / 3\n",
        "\n",
        "    return T\n",
        "    \n",
        "  # [Action required]\n",
        "  def fill_in_reward(self):\n",
        "    \"\"\"\n",
        "    Compute the reward matrix of the grid\n",
        "    input: /\n",
        "    output: R {np.array} -- the reward matrix of the grid\n",
        "    \"\"\"\n",
        "    R = np.ones((self.state_size, self.state_size, self.action_size)) # Matrix filled with 1\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.absorbing_rewards, self.absorbing_locs, self.get_state_from_loc()\n",
        "    ####\n",
        "    \n",
        "\n",
        "    return R\n",
        "\n",
        "\n",
        "\n",
        "  # [Action required]\n",
        "  def policy_evaluation(self, policy, threshold = 0.0001, gamma = 0.8):\n",
        "    \"\"\"\n",
        "    Policy evaluation on GridWorld\n",
        "    input: \n",
        "      - policy {np.array} -- policy to evaluate\n",
        "      - threshold {float} -- threshold value used to stop the policy evaluation algorithm\n",
        "      - gamma {float} -- discount factor\n",
        "    output: \n",
        "      - V {np.array} -- value function corresponding to the policy \n",
        "      - epochs {int} -- number of epochs to find this value function\n",
        "    \"\"\"\n",
        "    \n",
        "    # Ensure inputs are valid\n",
        "    assert (policy.shape[0] == self.state_size) and (policy.shape[1] == self.action_size), \"The dimensions of the policy are not valid.\"\n",
        "    assert (gamma <=1) and (gamma >= 0), \"Discount factor should be in [0, 1].\"\n",
        "\n",
        "    # Initialisation\n",
        "    delta = 2*threshold # Ensure delta is bigger than the threshold to start the loop\n",
        "    V = np.zeros(self.state_size) # Initialise value function to 0  \n",
        "    epoch = 0\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.state_size, self.action_size, self.T, self.R, self.absorbing\n",
        "    ####\n",
        "            \n",
        "    return V, epoch\n",
        "\n",
        "  # [Action required]\n",
        "  def policy_iteration(self, threshold = 0.0001, gamma = 0.8):\n",
        "    \"\"\"\n",
        "    Policy iteration on GridWorld\n",
        "    input: \n",
        "      - threshold {float} -- threshold value used to stop the policy iteration algorithm\n",
        "      - gamma {float} -- discount factor\n",
        "    output:\n",
        "      - policy {np.array} -- policy found using the policy iteration algorithm\n",
        "      - V {np.array} -- value function corresponding to the policy \n",
        "      - epochs {int} -- number of epochs to find this policy\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure gamma value is valid\n",
        "    assert (gamma <=1) and (gamma >= 0), \"Discount factor should be in [0, 1].\"\n",
        "\n",
        "    # Initialisation\n",
        "    policy = np.zeros((self.state_size, self.action_size)) # Vector of 0\n",
        "    policy[:, 0] = 1 # Initialise policy to choose action 1 systematically\n",
        "    V = np.zeros(self.state_size) # Initialise value function to 0  \n",
        "    epochs = 0\n",
        "    policy_stable = False # Condition to stop the main loop\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.state_size, self.action_size, self.T, self.R, self.absorbing, self.policy_evaluation()\n",
        "    ####\n",
        "            \n",
        "    return policy, V, epochs\n",
        " \n",
        "  # [Action required]\n",
        "  def value_iteration(self, threshold = 0.0001, gamma = 0.8):\n",
        "    \"\"\"\n",
        "    Value iteration on GridWorld\n",
        "    input: \n",
        "      - threshold {float} -- threshold value used to stop the value iteration algorithm\n",
        "      - gamma {float} -- discount factor\n",
        "    output: \n",
        "      - policy {np.array} -- optimal policy found using the value iteration algorithm\n",
        "      - V {np.array} -- value function corresponding to the policy\n",
        "      - epochs {int} -- number of epochs to find this policy\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure gamma value is valid\n",
        "    assert (gamma <=1) and (gamma >= 0), \"Discount factor should be in [0, 1].\"\n",
        "\n",
        "    # Initialisation\n",
        "    epochs = 0\n",
        "    delta = threshold # Setting value of delta to go through the first breaking condition\n",
        "    V = np.zeros(self.state_size) # Initialise values at 0 for each state\n",
        "    policy = np.zeros((self.state_size, self.action_size)) # Initialisation\n",
        "\n",
        "    #### \n",
        "    # Add your code here\n",
        "    # Hint! You might need: self.state_size, self.action_size, self.T, self.R, self.absorbing\n",
        "    ####\n",
        "\n",
        "    return policy, V, epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJZHGUz55V84"
      },
      "source": [
        "## Question 1: Grid World Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "button": false,
        "id": "eyeJfvwXp3ta",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating the Grid world:\n",
            "\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 0\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 1\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 2\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 3\n",
            "State: 0\n",
            "Neighbour: 1.0\n",
            "Action: 0\n",
            "State: 0\n",
            "Neighbour: 1.0\n",
            "Action: 1\n",
            "State: 0\n",
            "Neighbour: 1.0\n",
            "Action: 2\n",
            "State: 0\n",
            "Neighbour: 1.0\n",
            "Action: 3\n",
            "State: 0\n",
            "Neighbour: 5.0\n",
            "Action: 0\n",
            "State: 0\n",
            "Neighbour: 5.0\n",
            "Action: 1\n",
            "State: 0\n",
            "Neighbour: 5.0\n",
            "Action: 2\n",
            "State: 0\n",
            "Neighbour: 5.0\n",
            "Action: 3\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 0\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 1\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 2\n",
            "State: 0\n",
            "Neighbour: 0.0\n",
            "Action: 3\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 0\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 1\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 2\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 3\n",
            "State: 1\n",
            "Neighbour: 2.0\n",
            "Action: 0\n",
            "State: 1\n",
            "Neighbour: 2.0\n",
            "Action: 1\n",
            "State: 1\n",
            "Neighbour: 2.0\n",
            "Action: 2\n",
            "State: 1\n",
            "Neighbour: 2.0\n",
            "Action: 3\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 0\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 1\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 2\n",
            "State: 1\n",
            "Neighbour: 1.0\n",
            "Action: 3\n",
            "State: 1\n",
            "Neighbour: 0.0\n",
            "Action: 0\n",
            "State: 1\n",
            "Neighbour: 0.0\n",
            "Action: 1\n",
            "State: 1\n",
            "Neighbour: 0.0\n",
            "Action: 2\n",
            "State: 1\n",
            "Neighbour: 0.0\n",
            "Action: 3\n",
            "State: 2\n",
            "Neighbour: 2.0\n",
            "Action: 0\n",
            "State: 2\n",
            "Neighbour: 2.0\n",
            "Action: 1\n",
            "State: 2\n",
            "Neighbour: 2.0\n",
            "Action: 2\n",
            "State: 2\n",
            "Neighbour: 2.0\n",
            "Action: 3\n",
            "State: 2\n",
            "Neighbour: 3.0\n",
            "Action: 0\n",
            "State: 2\n",
            "Neighbour: 3.0\n",
            "Action: 1\n",
            "State: 2\n",
            "Neighbour: 3.0\n",
            "Action: 2\n",
            "State: 2\n",
            "Neighbour: 3.0\n",
            "Action: 3\n",
            "State: 2\n",
            "Neighbour: 6.0\n",
            "Action: 0\n",
            "State: 2\n",
            "Neighbour: 6.0\n",
            "Action: 1\n",
            "State: 2\n",
            "Neighbour: 6.0\n",
            "Action: 2\n",
            "State: 2\n",
            "Neighbour: 6.0\n",
            "Action: 3\n",
            "State: 2\n",
            "Neighbour: 1.0\n",
            "Action: 0\n",
            "State: 2\n",
            "Neighbour: 1.0\n",
            "Action: 1\n",
            "State: 2\n",
            "Neighbour: 1.0\n",
            "Action: 2\n",
            "State: 2\n",
            "Neighbour: 1.0\n",
            "Action: 3\n",
            "State: 3\n",
            "Neighbour: 3.0\n",
            "Action: 0\n",
            "State: 3\n",
            "Neighbour: 3.0\n",
            "Action: 1\n",
            "State: 3\n",
            "Neighbour: 3.0\n",
            "Action: 2\n",
            "State: 3\n",
            "Neighbour: 3.0\n",
            "Action: 3\n",
            "State: 3\n",
            "Neighbour: 4.0\n",
            "Action: 0\n",
            "State: 3\n",
            "Neighbour: 4.0\n",
            "Action: 1\n",
            "State: 3\n",
            "Neighbour: 4.0\n",
            "Action: 2\n",
            "State: 3\n",
            "Neighbour: 4.0\n",
            "Action: 3\n",
            "State: 3\n",
            "Neighbour: 7.0\n",
            "Action: 0\n",
            "State: 3\n",
            "Neighbour: 7.0\n",
            "Action: 1\n",
            "State: 3\n",
            "Neighbour: 7.0\n",
            "Action: 2\n",
            "State: 3\n",
            "Neighbour: 7.0\n",
            "Action: 3\n",
            "State: 3\n",
            "Neighbour: 2.0\n",
            "Action: 0\n",
            "State: 3\n",
            "Neighbour: 2.0\n",
            "Action: 1\n",
            "State: 3\n",
            "Neighbour: 2.0\n",
            "Action: 2\n",
            "State: 3\n",
            "Neighbour: 2.0\n",
            "Action: 3\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 0\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 1\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 2\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 3\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 0\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 1\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 2\n",
            "State: 4\n",
            "Neighbour: 4.0\n",
            "Action: 3\n",
            "State: 4\n",
            "Neighbour: 8.0\n",
            "Action: 0\n",
            "State: 4\n",
            "Neighbour: 8.0\n",
            "Action: 1\n",
            "State: 4\n",
            "Neighbour: 8.0\n",
            "Action: 2\n",
            "State: 4\n",
            "Neighbour: 8.0\n",
            "Action: 3\n",
            "State: 4\n",
            "Neighbour: 3.0\n",
            "Action: 0\n",
            "State: 4\n",
            "Neighbour: 3.0\n",
            "Action: 1\n",
            "State: 4\n",
            "Neighbour: 3.0\n",
            "Action: 2\n",
            "State: 4\n",
            "Neighbour: 3.0\n",
            "Action: 3\n",
            "State: 5\n",
            "Neighbour: 0.0\n",
            "Action: 0\n",
            "State: 5\n",
            "Neighbour: 0.0\n",
            "Action: 1\n",
            "State: 5\n",
            "Neighbour: 0.0\n",
            "Action: 2\n",
            "State: 5\n",
            "Neighbour: 0.0\n",
            "Action: 3\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 0\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 1\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 2\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 3\n",
            "State: 5\n",
            "Neighbour: 9.0\n",
            "Action: 0\n",
            "State: 5\n",
            "Neighbour: 9.0\n",
            "Action: 1\n",
            "State: 5\n",
            "Neighbour: 9.0\n",
            "Action: 2\n",
            "State: 5\n",
            "Neighbour: 9.0\n",
            "Action: 3\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 0\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 1\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 2\n",
            "State: 5\n",
            "Neighbour: 5.0\n",
            "Action: 3\n",
            "State: 6\n",
            "Neighbour: 2.0\n",
            "Action: 0\n",
            "State: 6\n",
            "Neighbour: 2.0\n",
            "Action: 1\n",
            "State: 6\n",
            "Neighbour: 2.0\n",
            "Action: 2\n",
            "State: 6\n",
            "Neighbour: 2.0\n",
            "Action: 3\n",
            "State: 6\n",
            "Neighbour: 7.0\n",
            "Action: 0\n",
            "State: 6\n",
            "Neighbour: 7.0\n",
            "Action: 1\n",
            "State: 6\n",
            "Neighbour: 7.0\n",
            "Action: 2\n",
            "State: 6\n",
            "Neighbour: 7.0\n",
            "Action: 3\n",
            "State: 6\n",
            "Neighbour: 10.0\n",
            "Action: 0\n",
            "State: 6\n",
            "Neighbour: 10.0\n",
            "Action: 1\n",
            "State: 6\n",
            "Neighbour: 10.0\n",
            "Action: 2\n",
            "State: 6\n",
            "Neighbour: 10.0\n",
            "Action: 3\n",
            "State: 6\n",
            "Neighbour: 6.0\n",
            "Action: 0\n",
            "State: 6\n",
            "Neighbour: 6.0\n",
            "Action: 1\n",
            "State: 6\n",
            "Neighbour: 6.0\n",
            "Action: 2\n",
            "State: 6\n",
            "Neighbour: 6.0\n",
            "Action: 3\n",
            "State: 7\n",
            "Neighbour: 3.0\n",
            "Action: 0\n",
            "State: 7\n",
            "Neighbour: 3.0\n",
            "Action: 1\n",
            "State: 7\n",
            "Neighbour: 3.0\n",
            "Action: 2\n",
            "State: 7\n",
            "Neighbour: 3.0\n",
            "Action: 3\n",
            "State: 7\n",
            "Neighbour: 8.0\n",
            "Action: 0\n",
            "State: 7\n",
            "Neighbour: 8.0\n",
            "Action: 1\n",
            "State: 7\n",
            "Neighbour: 8.0\n",
            "Action: 2\n",
            "State: 7\n",
            "Neighbour: 8.0\n",
            "Action: 3\n",
            "State: 7\n",
            "Neighbour: 7.0\n",
            "Action: 0\n",
            "State: 7\n",
            "Neighbour: 7.0\n",
            "Action: 1\n",
            "State: 7\n",
            "Neighbour: 7.0\n",
            "Action: 2\n",
            "State: 7\n",
            "Neighbour: 7.0\n",
            "Action: 3\n",
            "State: 7\n",
            "Neighbour: 6.0\n",
            "Action: 0\n",
            "State: 7\n",
            "Neighbour: 6.0\n",
            "Action: 1\n",
            "State: 7\n",
            "Neighbour: 6.0\n",
            "Action: 2\n",
            "State: 7\n",
            "Neighbour: 6.0\n",
            "Action: 3\n",
            "State: 8\n",
            "Neighbour: 4.0\n",
            "Action: 0\n",
            "State: 8\n",
            "Neighbour: 4.0\n",
            "Action: 1\n",
            "State: 8\n",
            "Neighbour: 4.0\n",
            "Action: 2\n",
            "State: 8\n",
            "Neighbour: 4.0\n",
            "Action: 3\n",
            "State: 8\n",
            "Neighbour: 8.0\n",
            "Action: 0\n",
            "State: 8\n",
            "Neighbour: 8.0\n",
            "Action: 1\n",
            "State: 8\n",
            "Neighbour: 8.0\n",
            "Action: 2\n",
            "State: 8\n",
            "Neighbour: 8.0\n",
            "Action: 3\n",
            "State: 8\n",
            "Neighbour: 11.0\n",
            "Action: 0\n",
            "State: 8\n",
            "Neighbour: 11.0\n",
            "Action: 1\n",
            "State: 8\n",
            "Neighbour: 11.0\n",
            "Action: 2\n",
            "State: 8\n",
            "Neighbour: 11.0\n",
            "Action: 3\n",
            "State: 8\n",
            "Neighbour: 7.0\n",
            "Action: 0\n",
            "State: 8\n",
            "Neighbour: 7.0\n",
            "Action: 1\n",
            "State: 8\n",
            "Neighbour: 7.0\n",
            "Action: 2\n",
            "State: 8\n",
            "Neighbour: 7.0\n",
            "Action: 3\n",
            "State: 9\n",
            "Neighbour: 5.0\n",
            "Action: 0\n",
            "State: 9\n",
            "Neighbour: 5.0\n",
            "Action: 1\n",
            "State: 9\n",
            "Neighbour: 5.0\n",
            "Action: 2\n",
            "State: 9\n",
            "Neighbour: 5.0\n",
            "Action: 3\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 0\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 1\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 2\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 3\n",
            "State: 9\n",
            "Neighbour: 12.0\n",
            "Action: 0\n",
            "State: 9\n",
            "Neighbour: 12.0\n",
            "Action: 1\n",
            "State: 9\n",
            "Neighbour: 12.0\n",
            "Action: 2\n",
            "State: 9\n",
            "Neighbour: 12.0\n",
            "Action: 3\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 0\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 1\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 2\n",
            "State: 9\n",
            "Neighbour: 9.0\n",
            "Action: 3\n",
            "State: 10\n",
            "Neighbour: 6.0\n",
            "Action: 0\n",
            "State: 10\n",
            "Neighbour: 6.0\n",
            "Action: 1\n",
            "State: 10\n",
            "Neighbour: 6.0\n",
            "Action: 2\n",
            "State: 10\n",
            "Neighbour: 6.0\n",
            "Action: 3\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 0\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 1\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 2\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 3\n",
            "State: 10\n",
            "Neighbour: 14.0\n",
            "Action: 0\n",
            "State: 10\n",
            "Neighbour: 14.0\n",
            "Action: 1\n",
            "State: 10\n",
            "Neighbour: 14.0\n",
            "Action: 2\n",
            "State: 10\n",
            "Neighbour: 14.0\n",
            "Action: 3\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 0\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 1\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 2\n",
            "State: 10\n",
            "Neighbour: 10.0\n",
            "Action: 3\n",
            "State: 11\n",
            "Neighbour: 8.0\n",
            "Action: 0\n",
            "State: 11\n",
            "Neighbour: 8.0\n",
            "Action: 1\n",
            "State: 11\n",
            "Neighbour: 8.0\n",
            "Action: 2\n",
            "State: 11\n",
            "Neighbour: 8.0\n",
            "Action: 3\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 0\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 1\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 2\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 3\n",
            "State: 11\n",
            "Neighbour: 16.0\n",
            "Action: 0\n",
            "State: 11\n",
            "Neighbour: 16.0\n",
            "Action: 1\n",
            "State: 11\n",
            "Neighbour: 16.0\n",
            "Action: 2\n",
            "State: 11\n",
            "Neighbour: 16.0\n",
            "Action: 3\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 0\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 1\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 2\n",
            "State: 11\n",
            "Neighbour: 11.0\n",
            "Action: 3\n",
            "State: 12\n",
            "Neighbour: 9.0\n",
            "Action: 0\n",
            "State: 12\n",
            "Neighbour: 9.0\n",
            "Action: 1\n",
            "State: 12\n",
            "Neighbour: 9.0\n",
            "Action: 2\n",
            "State: 12\n",
            "Neighbour: 9.0\n",
            "Action: 3\n",
            "State: 12\n",
            "Neighbour: 13.0\n",
            "Action: 0\n",
            "State: 12\n",
            "Neighbour: 13.0\n",
            "Action: 1\n",
            "State: 12\n",
            "Neighbour: 13.0\n",
            "Action: 2\n",
            "State: 12\n",
            "Neighbour: 13.0\n",
            "Action: 3\n",
            "State: 12\n",
            "Neighbour: 17.0\n",
            "Action: 0\n",
            "State: 12\n",
            "Neighbour: 17.0\n",
            "Action: 1\n",
            "State: 12\n",
            "Neighbour: 17.0\n",
            "Action: 2\n",
            "State: 12\n",
            "Neighbour: 17.0\n",
            "Action: 3\n",
            "State: 12\n",
            "Neighbour: 12.0\n",
            "Action: 0\n",
            "State: 12\n",
            "Neighbour: 12.0\n",
            "Action: 1\n",
            "State: 12\n",
            "Neighbour: 12.0\n",
            "Action: 2\n",
            "State: 12\n",
            "Neighbour: 12.0\n",
            "Action: 3\n",
            "State: 13\n",
            "Neighbour: 13.0\n",
            "Action: 0\n",
            "State: 13\n",
            "Neighbour: 13.0\n",
            "Action: 1\n",
            "State: 13\n",
            "Neighbour: 13.0\n",
            "Action: 2\n",
            "State: 13\n",
            "Neighbour: 13.0\n",
            "Action: 3\n",
            "State: 13\n",
            "Neighbour: 14.0\n",
            "Action: 0\n",
            "State: 13\n",
            "Neighbour: 14.0\n",
            "Action: 1\n",
            "State: 13\n",
            "Neighbour: 14.0\n",
            "Action: 2\n",
            "State: 13\n",
            "Neighbour: 14.0\n",
            "Action: 3\n",
            "State: 13\n",
            "Neighbour: 18.0\n",
            "Action: 0\n",
            "State: 13\n",
            "Neighbour: 18.0\n",
            "Action: 1\n",
            "State: 13\n",
            "Neighbour: 18.0\n",
            "Action: 2\n",
            "State: 13\n",
            "Neighbour: 18.0\n",
            "Action: 3\n",
            "State: 13\n",
            "Neighbour: 12.0\n",
            "Action: 0\n",
            "State: 13\n",
            "Neighbour: 12.0\n",
            "Action: 1\n",
            "State: 13\n",
            "Neighbour: 12.0\n",
            "Action: 2\n",
            "State: 13\n",
            "Neighbour: 12.0\n",
            "Action: 3\n",
            "State: 14\n",
            "Neighbour: 10.0\n",
            "Action: 0\n",
            "State: 14\n",
            "Neighbour: 10.0\n",
            "Action: 1\n",
            "State: 14\n",
            "Neighbour: 10.0\n",
            "Action: 2\n",
            "State: 14\n",
            "Neighbour: 10.0\n",
            "Action: 3\n",
            "State: 14\n",
            "Neighbour: 15.0\n",
            "Action: 0\n",
            "State: 14\n",
            "Neighbour: 15.0\n",
            "Action: 1\n",
            "State: 14\n",
            "Neighbour: 15.0\n",
            "Action: 2\n",
            "State: 14\n",
            "Neighbour: 15.0\n",
            "Action: 3\n",
            "State: 14\n",
            "Neighbour: 19.0\n",
            "Action: 0\n",
            "State: 14\n",
            "Neighbour: 19.0\n",
            "Action: 1\n",
            "State: 14\n",
            "Neighbour: 19.0\n",
            "Action: 2\n",
            "State: 14\n",
            "Neighbour: 19.0\n",
            "Action: 3\n",
            "State: 14\n",
            "Neighbour: 13.0\n",
            "Action: 0\n",
            "State: 14\n",
            "Neighbour: 13.0\n",
            "Action: 1\n",
            "State: 14\n",
            "Neighbour: 13.0\n",
            "Action: 2\n",
            "State: 14\n",
            "Neighbour: 13.0\n",
            "Action: 3\n",
            "State: 15\n",
            "Neighbour: 15.0\n",
            "Action: 0\n",
            "State: 15\n",
            "Neighbour: 15.0\n",
            "Action: 1\n",
            "State: 15\n",
            "Neighbour: 15.0\n",
            "Action: 2\n",
            "State: 15\n",
            "Neighbour: 15.0\n",
            "Action: 3\n",
            "State: 15\n",
            "Neighbour: 16.0\n",
            "Action: 0\n",
            "State: 15\n",
            "Neighbour: 16.0\n",
            "Action: 1\n",
            "State: 15\n",
            "Neighbour: 16.0\n",
            "Action: 2\n",
            "State: 15\n",
            "Neighbour: 16.0\n",
            "Action: 3\n",
            "State: 15\n",
            "Neighbour: 20.0\n",
            "Action: 0\n",
            "State: 15\n",
            "Neighbour: 20.0\n",
            "Action: 1\n",
            "State: 15\n",
            "Neighbour: 20.0\n",
            "Action: 2\n",
            "State: 15\n",
            "Neighbour: 20.0\n",
            "Action: 3\n",
            "State: 15\n",
            "Neighbour: 14.0\n",
            "Action: 0\n",
            "State: 15\n",
            "Neighbour: 14.0\n",
            "Action: 1\n",
            "State: 15\n",
            "Neighbour: 14.0\n",
            "Action: 2\n",
            "State: 15\n",
            "Neighbour: 14.0\n",
            "Action: 3\n",
            "State: 16\n",
            "Neighbour: 11.0\n",
            "Action: 0\n",
            "State: 16\n",
            "Neighbour: 11.0\n",
            "Action: 1\n",
            "State: 16\n",
            "Neighbour: 11.0\n",
            "Action: 2\n",
            "State: 16\n",
            "Neighbour: 11.0\n",
            "Action: 3\n",
            "State: 16\n",
            "Neighbour: 16.0\n",
            "Action: 0\n",
            "State: 16\n",
            "Neighbour: 16.0\n",
            "Action: 1\n",
            "State: 16\n",
            "Neighbour: 16.0\n",
            "Action: 2\n",
            "State: 16\n",
            "Neighbour: 16.0\n",
            "Action: 3\n",
            "State: 16\n",
            "Neighbour: 21.0\n",
            "Action: 0\n",
            "State: 16\n",
            "Neighbour: 21.0\n",
            "Action: 1\n",
            "State: 16\n",
            "Neighbour: 21.0\n",
            "Action: 2\n",
            "State: 16\n",
            "Neighbour: 21.0\n",
            "Action: 3\n",
            "State: 16\n",
            "Neighbour: 15.0\n",
            "Action: 0\n",
            "State: 16\n",
            "Neighbour: 15.0\n",
            "Action: 1\n",
            "State: 16\n",
            "Neighbour: 15.0\n",
            "Action: 2\n",
            "State: 16\n",
            "Neighbour: 15.0\n",
            "Action: 3\n",
            "State: 17\n",
            "Neighbour: 12.0\n",
            "Action: 0\n",
            "State: 17\n",
            "Neighbour: 12.0\n",
            "Action: 1\n",
            "State: 17\n",
            "Neighbour: 12.0\n",
            "Action: 2\n",
            "State: 17\n",
            "Neighbour: 12.0\n",
            "Action: 3\n",
            "State: 17\n",
            "Neighbour: 18.0\n",
            "Action: 0\n",
            "State: 17\n",
            "Neighbour: 18.0\n",
            "Action: 1\n",
            "State: 17\n",
            "Neighbour: 18.0\n",
            "Action: 2\n",
            "State: 17\n",
            "Neighbour: 18.0\n",
            "Action: 3\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 0\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 1\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 2\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 3\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 0\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 1\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 2\n",
            "State: 17\n",
            "Neighbour: 17.0\n",
            "Action: 3\n",
            "State: 18\n",
            "Neighbour: 13.0\n",
            "Action: 0\n",
            "State: 18\n",
            "Neighbour: 13.0\n",
            "Action: 1\n",
            "State: 18\n",
            "Neighbour: 13.0\n",
            "Action: 2\n",
            "State: 18\n",
            "Neighbour: 13.0\n",
            "Action: 3\n",
            "State: 18\n",
            "Neighbour: 19.0\n",
            "Action: 0\n",
            "State: 18\n",
            "Neighbour: 19.0\n",
            "Action: 1\n",
            "State: 18\n",
            "Neighbour: 19.0\n",
            "Action: 2\n",
            "State: 18\n",
            "Neighbour: 19.0\n",
            "Action: 3\n",
            "State: 18\n",
            "Neighbour: 18.0\n",
            "Action: 0\n",
            "State: 18\n",
            "Neighbour: 18.0\n",
            "Action: 1\n",
            "State: 18\n",
            "Neighbour: 18.0\n",
            "Action: 2\n",
            "State: 18\n",
            "Neighbour: 18.0\n",
            "Action: 3\n",
            "State: 18\n",
            "Neighbour: 17.0\n",
            "Action: 0\n",
            "State: 18\n",
            "Neighbour: 17.0\n",
            "Action: 1\n",
            "State: 18\n",
            "Neighbour: 17.0\n",
            "Action: 2\n",
            "State: 18\n",
            "Neighbour: 17.0\n",
            "Action: 3\n",
            "State: 19\n",
            "Neighbour: 14.0\n",
            "Action: 0\n",
            "State: 19\n",
            "Neighbour: 14.0\n",
            "Action: 1\n",
            "State: 19\n",
            "Neighbour: 14.0\n",
            "Action: 2\n",
            "State: 19\n",
            "Neighbour: 14.0\n",
            "Action: 3\n",
            "State: 19\n",
            "Neighbour: 20.0\n",
            "Action: 0\n",
            "State: 19\n",
            "Neighbour: 20.0\n",
            "Action: 1\n",
            "State: 19\n",
            "Neighbour: 20.0\n",
            "Action: 2\n",
            "State: 19\n",
            "Neighbour: 20.0\n",
            "Action: 3\n",
            "State: 19\n",
            "Neighbour: 19.0\n",
            "Action: 0\n",
            "State: 19\n",
            "Neighbour: 19.0\n",
            "Action: 1\n",
            "State: 19\n",
            "Neighbour: 19.0\n",
            "Action: 2\n",
            "State: 19\n",
            "Neighbour: 19.0\n",
            "Action: 3\n",
            "State: 19\n",
            "Neighbour: 18.0\n",
            "Action: 0\n",
            "State: 19\n",
            "Neighbour: 18.0\n",
            "Action: 1\n",
            "State: 19\n",
            "Neighbour: 18.0\n",
            "Action: 2\n",
            "State: 19\n",
            "Neighbour: 18.0\n",
            "Action: 3\n",
            "State: 20\n",
            "Neighbour: 15.0\n",
            "Action: 0\n",
            "State: 20\n",
            "Neighbour: 15.0\n",
            "Action: 1\n",
            "State: 20\n",
            "Neighbour: 15.0\n",
            "Action: 2\n",
            "State: 20\n",
            "Neighbour: 15.0\n",
            "Action: 3\n",
            "State: 20\n",
            "Neighbour: 21.0\n",
            "Action: 0\n",
            "State: 20\n",
            "Neighbour: 21.0\n",
            "Action: 1\n",
            "State: 20\n",
            "Neighbour: 21.0\n",
            "Action: 2\n",
            "State: 20\n",
            "Neighbour: 21.0\n",
            "Action: 3\n",
            "State: 20\n",
            "Neighbour: 20.0\n",
            "Action: 0\n",
            "State: 20\n",
            "Neighbour: 20.0\n",
            "Action: 1\n",
            "State: 20\n",
            "Neighbour: 20.0\n",
            "Action: 2\n",
            "State: 20\n",
            "Neighbour: 20.0\n",
            "Action: 3\n",
            "State: 20\n",
            "Neighbour: 19.0\n",
            "Action: 0\n",
            "State: 20\n",
            "Neighbour: 19.0\n",
            "Action: 1\n",
            "State: 20\n",
            "Neighbour: 19.0\n",
            "Action: 2\n",
            "State: 20\n",
            "Neighbour: 19.0\n",
            "Action: 3\n",
            "State: 21\n",
            "Neighbour: 16.0\n",
            "Action: 0\n",
            "State: 21\n",
            "Neighbour: 16.0\n",
            "Action: 1\n",
            "State: 21\n",
            "Neighbour: 16.0\n",
            "Action: 2\n",
            "State: 21\n",
            "Neighbour: 16.0\n",
            "Action: 3\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 0\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 1\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 2\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 3\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 0\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 1\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 2\n",
            "State: 21\n",
            "Neighbour: 21.0\n",
            "Action: 3\n",
            "State: 21\n",
            "Neighbour: 20.0\n",
            "Action: 0\n",
            "State: 21\n",
            "Neighbour: 20.0\n",
            "Action: 1\n",
            "State: 21\n",
            "Neighbour: 20.0\n",
            "Action: 2\n",
            "State: 21\n",
            "Neighbour: 20.0\n",
            "Action: 3\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The first dimension of the transition matrix is: 22\n",
            "The second dimension of the transition matrix is: 22\n",
            "The third dimension of the transition matrix is: 4\n",
            "The probability to go from state 0 to state 1 with action 0 (N) is: 0.10000000000000002\n"
          ]
        }
      ],
      "source": [
        "### Question 1: Grid World definition\n",
        "\n",
        "# Define the grid\n",
        "print(\"Creating the Grid world:\\n\")\n",
        "grid = GridWorld()\n",
        "\n",
        "# Exemple prints to help you understand the structure of the transition matrix\n",
        "print(\"\\nThe first dimension of the transition matrix is:\", len(grid.T))\n",
        "print(\"The second dimension of the transition matrix is:\", len(grid.T[0]))\n",
        "print(\"The third dimension of the transition matrix is:\", len(grid.T[0,0]))\n",
        "print(\"The probability to go from state 0 to state 1 with action 0 (N) is:\", grid.T[0,1,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0saHSxku5gMQ"
      },
      "source": [
        "## Question 2: policy evaluation implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "id": "f3238vrZp3ti",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "### Question 2: policy evaluation implementation\n",
        "\n",
        "# Define the policy\n",
        "Policy = np.zeros((grid.state_size, grid.action_size))\n",
        "Policy = Policy + 0.25\n",
        "\n",
        "# Do not plot a graphical representation for this policy as it is fully random\n",
        "#print(\"Considering the uniform (unbiased) policy:\\n\\n {}\".format(Policy))\n",
        "\n",
        "# Policy evaluation with threshold = 0.001 and gamma = 0.8\n",
        "V, epochs = grid.policy_evaluation(Policy, threshold = 0.001, gamma = 0.8)\n",
        "\n",
        "# Plot value function\n",
        "print(\"The value of the uniform policy with gamma = 0.8 is:\\n\\n {}\".format(V))\n",
        "print(\"\\n\\nIts graphical representation is:\\n\")\n",
        "grid.graphics.draw_value(V)\n",
        "\n",
        "# Plot number of epochs\n",
        "print(\"\\nIt took {} epochs\".format(epochs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk1oDM825oPE"
      },
      "source": [
        "## Question 3: impact of gamma on the policy evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "id": "VF4ilbcrp3tg",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "### Question 3: impact of gamma on the policy evaluation\n",
        "\n",
        "gamma_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "epochs = []\n",
        "values = []\n",
        "titles = []\n",
        "\n",
        "# Use policy evaluation for each gamma value\n",
        "for gamma in gamma_range:\n",
        "    V, epoch = grid.policy_evaluation(Policy, threshold = 0.001, gamma = gamma)\n",
        "    epochs.append(epoch)\n",
        "    values.append(V)\n",
        "    titles.append(\"gamma = {}\".format(gamma))\n",
        "\n",
        "# Plot the number of epochs vs gamma values\n",
        "print(\"Impact of gamma value on the number of epochs needed for the policy evaluation algorithm:\\n\")\n",
        "plt.figure()\n",
        "plt.plot(gamma_range, epochs)\n",
        "plt.xlabel(\"Gamma\")\n",
        "plt.ylabel(\"Number of epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Print all value functions for different values of gamma\n",
        "print(\"\\nGraphical representation of the value function for each gamma:\\n\")\n",
        "grid.graphics.draw_value_grid(values, titles, 2, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFuN72Li5sVA"
      },
      "source": [
        "## Question 4: policy iteration implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "id": "gsK5TzK3p3tl",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "### Question 4: policy iteration implementation\n",
        "\n",
        "# Policy iteration algorithm\n",
        "policy, V, epochs = grid.policy_iteration(threshold = 0.001, gamma = 0.7)\n",
        "\n",
        "# Plot value function for policy iteration\n",
        "#print(\"The value of the optimal value computed using policy iteration is:\\n\\n {}\\n\\n\".format(V))\n",
        "print(\"The graphical representation of the optimal value computed using policy iteration is:\\n\")\n",
        "grid.graphics.draw_value(V)\n",
        "\n",
        "# Plot policy for policy iteration\n",
        "#print(\"\\n\\nThe optimal policy using policy iteration is:\\n\\n {}\\n\\n\".format(policy))\n",
        "print(\"The graphical representation of the optimal policy using policy iteration is:\\n\")\n",
        "grid.graphics.draw_policy(policy)\n",
        "\n",
        "# Plot number of epochs\n",
        "print(\"\\nIt took {} epochs\".format(epochs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suov-nzD5v6e"
      },
      "source": [
        "## Question 5:  impact of gamma on the policy iteration algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJP49aeCy8e9"
      },
      "outputs": [],
      "source": [
        "### Question 5:  impact of gamma on the policy iteration algorithm\n",
        "\n",
        "gamma_range = [0, 0.2, 0.4, 0.6, 0.8]\n",
        "epochs = []\n",
        "policies = []\n",
        "values = []\n",
        "titles = []\n",
        "\n",
        "# Use policy iteration for each gamma value\n",
        "for gamma in gamma_range:\n",
        "    policy, V, epoch = grid.policy_iteration(threshold = 0.001, gamma = gamma)\n",
        "    epochs.append(epoch)\n",
        "    policies.append(policy)\n",
        "    values.append(V)\n",
        "    titles.append(\"gamma = {}\".format(gamma))\n",
        "\n",
        "# Plot the number of epochs vs gamma values\n",
        "print(\"Impact of gamma value on the number of epochs needed for the policy iteration algorithm:\\n\")\n",
        "plt.figure()\n",
        "plt.plot(gamma_range, epochs)\n",
        "plt.xlabel(\"Gamma range\")\n",
        "plt.ylabel(\"Number of epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Print all value functions and policies for different values of gamma\n",
        "print(\"\\nGraphical representation of the value function for each gamma:\\n\")\n",
        "grid.graphics.draw_value_grid(values, titles, 1, 6)\n",
        "\n",
        "print(\"\\nGraphical representation of the policy for each gamma:\\n\")\n",
        "grid.graphics.draw_policy_grid(policies, titles, 1, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-hrUgit5zu9"
      },
      "source": [
        "## Question 6: value iteration implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9LOSONzq2_p"
      },
      "outputs": [],
      "source": [
        "### Question 6: value iteration implementation\n",
        "policy, V, epochs = grid.value_iteration(threshold = 0.001, gamma = 0.7)\n",
        "\n",
        "# Plot value function for policy iteration\n",
        "#print(\"The value of the optimal policy computed using value iteration is:\\n\\n {}\\n\\n\".format(V))\n",
        "print(\"The graphical representation of the value of the optimal policy computed using value iteration is:\\n\")\n",
        "grid.graphics.draw_value(V)\n",
        "\n",
        "# Plot policy for value iteration\n",
        "#print(\"\\n\\nThe optimal policy computed using value iteration is:\\n\\n {}\\n\\n\".format(policy))\n",
        "print(\"The graphical representation of the optimal policy computed using value iteration is:\\n\")\n",
        "grid.graphics.draw_policy(policy)\n",
        "\n",
        "# Plot number of epoch\n",
        "print(\"\\nIt took {} epochs\".format(epochs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPkJeiMg52ev"
      },
      "source": [
        "## Question 7:  impact of gamma on the value iteration algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqU9MinAy7gq"
      },
      "outputs": [],
      "source": [
        "### Question 7:  impact of gamma on the value iteration algorithm\n",
        "gamma_range = [0, 0.2, 0.4, 0.6, 0.8]\n",
        "epochs = []\n",
        "policies = []\n",
        "values = []\n",
        "titles = []\n",
        "\n",
        "# Use value iteration for each gamma value\n",
        "for gamma in gamma_range:\n",
        "    policy, V, epoch = grid.value_iteration(threshold = 0.001, gamma = gamma)\n",
        "    epochs.append(epoch)\n",
        "    policies.append(policy)\n",
        "    values.append(V)\n",
        "    titles.append(\"gamma = {}\".format(gamma))\n",
        "\n",
        "# Plot the number of epochs vs gamma values\n",
        "print(\"Impact of gamma value on the number of epochs needed for the value iteration algorithm:\\n\")\n",
        "plt.figure()\n",
        "plt.plot(gamma_range, epochs)\n",
        "plt.xlabel(\"Gamma range\")\n",
        "plt.ylabel(\"Number of epochs\")\n",
        "plt.show()\n",
        "\n",
        "# Print all value functions and policies for different values of gamma\n",
        "print(\"\\nGraphical representation of the value function for each gamma:\\n\")\n",
        "grid.graphics.draw_value_grid(values, titles, 1, 6)\n",
        "\n",
        "print(\"\\nGraphical representation of the policy for each gamma:\\n\")\n",
        "grid.graphics.draw_policy_grid(policies, titles, 1, 6)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "BIYexixg588i",
        "XOqA-RtjUn-_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rl-labs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
